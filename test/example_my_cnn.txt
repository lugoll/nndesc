Das Gefaltete Neuronal Netzwerk beginnt mit einer Faltungschicht mit einer Kernelgröße von 3x3 mit 32 Filtern, auf die eine ReLU als Aktivierungsfunktion angewendet wird,
gefolgt von einer weiteren Faltungsschicht mit der gleichen Kernelgröße sowie 64 Filtern mit einer ReLU als Aktivierungsfunktion.
Auf Beide Faltungschichten folgt für ein Downsampling jeweils eine MaxPool Schicht.
Zuletzt folgt eine vollverbunden Schicht mit eine Softmax Aktivierungsfunktion.





few shot:

The first layer of the Convolutional Neural Network is a Convolutional Layer with 32 filters of the size 3x3 and a ReLU activation function. The second layer is a maxpool layer for downsampling. The third layer is a Convolutional Layer with 64 filters of the size 3x3 and a ReLU activation function. The fourth layer is a maxpool layer for downsampling. The fifth layer is a Flatten layer. The last layer is a Dense Layer with a Softmax activation function.


The convolutional neural network starts with a convolutional layer with a kernel size of 3x3 with 32 filters to which a ReLU is applied as activation function, followed by another convolutional layer with the same kernel size and 64 filters also with a ReLU as activation function. Both convolutional layers are followed by a MaxPool layer for downsampling. Finally, a fully connected layer with a Softmax activation function follows.


The first layer of the Convolutional Neural Network is a Convolutional Layer with 16 filters of the size 5x5 and a Sigmoid activation function. The second layer is a maxpool layer for downsampling. The third layer is a Convolutional Layer with 16 filters of the size 5x5 and a Sigmoid activation function. The fourth layer is a maxpool layer for downsampling. The fifth layer is a Flatten layer. The last layer is a Dense Layer with a Softmax activation function.
